You have successfully tackled Performance Tuning! Now, let's move on to the essential topic of **Moving Data**, which involves using Directory Objects for security and two major utilities: Data Pump (modern logical migration) and SQL\*Loader (bulk loading).

### **Unit 8.0: Moving Data Guide (Directory Objects, Data Pump, SQL\*Loader, External Tables)**

### I. Create Directory Objects (Foundation)

Directory objects are foundational because Data Pump, SQL\*Loader, and External Tables all require them to operate safely and efficiently.

| Aspect | Description | Practical Use | Source |
| :--- | :--- | :--- | :--- |
| **Definition** | A schema object that serves as a logical alias for a physical directory path on the operating system (OS). | Provides a secure, centralized way to manage file access. | |
| **Why Use** | Abstracts the physical location of files. Required because Oracle server processes (like Data Pump workers) write dump files and log files to disk. | |
| **Required Privilege** | `CREATE ANY DIRECTORY` is required to create it. | | |
| **Creation Code** | You must ensure the OS directory path physically exists and is accessible (read/write) by the Oracle OS user. | `CREATE OR REPLACE DIRECTORY data_dir AS '/u01/app/oracle/data';` | |
| **Granting Access** | The user running the Data Pump or SQL\*Loader job must have specific privileges on the directory object. | `GRANT READ, WRITE ON DIRECTORY data_dir TO HR_USER;` | |
| **Viewing** | Directory objects can be viewed using the `DBA_DIRECTORIES` or `ALL_DIRECTORIES` data dictionary views. | `SELECT directory_name, directory_path FROM all_directories;` | |

### II. Data Pump (EXPDP / IMPDP)

Data Pump (`expdp` for export, `impdp` for import) is the modern, high-performance utility for moving data and metadata, replacing the older `exp`/`imp` utilities.

#### A. Data Pump Architecture and Data Dictionary Views

Data Pump runs as a server process, offering performance advantages and allowing client processes to disconnect and later reattach to the job.

| Component | Description | Source |
| :--- | :--- | :--- |
| **Clients** | `expdp` (unload data/metadata) and `impdp` (load data/metadata). | |
| **Job Coordinator** | Manages the overall job and coordinates between the master table and worker processes. | |
| **Master Table** | Central to Data Pump; tracks job state, progress, and object details; created in the schema of the user initiating the job. It is automatically dropped on successful completion. | |
| **Worker Processes** | Perform the actual data/metadata movement; multiple workers enable parallelization, enhancing performance. | |
| **Data Pump Views** | `DBA_DATAPUMP_JOBS` shows jobs running with an `EXECUTING` state. `DBA_DATAPUMP_SESSIONS` shows session information. | |

#### B. Data Pump API and Interactive Mode

The Data Pump utilities rely on the `DBMS_DATAPUMP` PL/SQL API for functionality. This allows interactive job management:

| Action | Command/Concept | Notes | Source |
| :--- | :--- | :--- | :--- |
| **Attach to Job** | Use the `ATTACH` keyword. | Allows monitoring and control if you disconnect from the original session. | |
| **Check Status** | `STATUS` command (interactive mode). | Shows job progress and current parallelism. | |
| **Terminate Job** | `KILL_JOB` command (interactive mode). | Immediately stops the job and drops the master table. | |
| **Pause/Restart** | `STOP_JOB=IMMEDIATE` for immediate shutdown. `STOP_JOB` performs an orderly shutdown and leaves the master table intact for a later restart. | `START_JOB` resumes the job. | |

#### C. Data Pump Export and Import (Practical Code)

| Action | Parameter/Mode | Practical Command Example | Source |
| :--- | :--- | :--- | :--- |
| **Export/Import Schema** | `SCHEMAS` (Default mode if unspecified). | `$ expdp user/pwd D=dp_dir DUMPFILE=sch.dmp SCHEMAS=hr,sales` | |
| **Export Metadata Only** | `CONTENT=METADATA_ONLY`. | `$ expdp user/pwd ... CONTENT=METADATA_ONLY` | |
| **Generate DDL** | `SQLFILE` (used during import). | `$ impdp user/pwd D=dp_dir DUMP=file.dmp SQLFILE=ddl.sql` | |
| **Remap Schema** | `REMAP_SCHEMA`. | `$ impdp user/pwd D=dmp F=file.dmp REMAP_SCHEMA=HR:HR_TEST` | |
| **Remap Tablespace** | `REMAP_TABLESPACE`. | `$ impdp user/pwd ... REMAP_TABLESPACE=OLD_TS:NEW_TS` | |
| **Transfer via Network** | `NETWORK_LINK`. | `$ impdp user/pwd D=dmp NL=remote_db SCHEMAS=hr` | |
| **Parallelize** | `PARALLEL=N`. | `$ expdp user/pwd D=dmp DUMPFILE=exp%U.dmp PARALLEL=4` | |
| **Handle Existing Table** | `TABLE_EXISTS_ACTION` (SKIP, APPEND, REPLACE, TRUNCATE). | `$ impdp user/pwd ... TABLE_EXISTS_ACTION=APPEND` | |
| **Use Parameter File** | `PARFILE`. | `$ expdp PARFILE=export.par` | |

### III. Overview of SQL\*Loader

SQL\*Loader is a utility used to load data from external flat files (data files) into database tables.

#### A. SQL\*Loader Components and Data Files

1.  **Data Files:** The source files containing the data to be loaded (e.g., plain text, CSV, delimited, or fixed-width formats).
2.  **Control File:** A text file containing Data Definition Language (DDL) instructions that dictate where to find the data, how the data is formatted, and how SQL\*Loader should be configured (e.g., memory, rejecting records).
3.  **Command Line Parameters:** Used to invoke the utility and specify key files (e.g., `USERID`, `CONTROL`, `DATA`, `LOG`, `PARFILE`).

#### B. Data Paths and Performance

| Path Type | Description | Performance Note | Source |
| :--- | :--- | :--- | :--- |
| **Conventional Path Load** | Default method. Executes SQL `INSERT` statements; uses a bind array buffer; competes for buffer resources. | Slower for bulk loads due to database overhead. | |
| **Direct Path Load** | Utilizes the Direct Path API to bypass the bind array and SQL `INSERT` statements; formats Oracle blocks directly and writes them to data files. Enabled via `DIRECT=TRUE`. | Avoids database overhead and competition; loads data near disk speed. | |

#### C. Control File Keywords and Record Filtering

The Control File defines the mapping of external data fields to internal table columns.

| Keyword/Clause | Purpose | Source |
| :--- | :--- | :--- |
| `LOAD DATA` | Indicates the start of a data load. | |
| `INFILE` | Specifies the input data file(s). | |
| `INTO TABLE` | Specifies the target table. | |
| `FIELDS TERMINATED BY` | Specifies the character that separates data fields (e.g., comma, pipe). | |
| `BADFILE` | File where records rejected due to errors are logged. | |
| `DISCARDFILE` | File where records that are neither inserted nor rejected (often due to `WHEN` clause failure) are stored. | |
| `WHEN Clause` | **Record Filtering:** Specifies a condition records must meet to be loaded; filtering occurs before other processing. | |

### IV. External Tables

External tables allow access to data in external files (like flat files or Data Pump dump files) using SQL, without loading the data into Oracle storage.

| Feature | Description | Source |
| :--- | :--- | :--- |
| **Core Principle** | Defined using `CREATE TABLE ... ORGANIZATION EXTERNAL`. To the user, querying an external table is like querying any other table. | |
| **Data Access** | **Unloading (Export):** Use `ORACLE_DATAPUMP` access driver with `AS SELECT` to create a platform-independent binary dump file. | |
| **Inline SQL** | Starting in 18c, data can be queried directly from a file using `SELECT columns FROM EXTERNAL` without creating a dictionary object. | |
| **Advantages** | Faster for large data sets (avoids loading overhead). Simplifies ETL by allowing direct SQL query and transformation. | |
| **Security/Efficiency**| Dump files created by external tables can be compressed and encrypted for efficient and secure transportation. | |

---

## Exam Practice Drill: Data Pump & Directory Objects

This is a critical practical scenario for your exam:

A developer needs to clone the objects and data owned by the schema `SALES_APP` into a new schema named `SALES_DEV` on the same database. The dump file must be stored in the directory specified by the path `/home/oracle/dumps`.

1.  Write the single SQL command required to create the necessary **Directory Object**, naming it `DP_EXPORT_DIR`, and map it to the file path.
2.  Write the Data Pump Export (`expdp`) command line that creates a full dump file (`sales.dmp`) containing only the `SALES_APP` schema, ensuring the file is placed in the correct location. Assume you are running the command as a DBA user who has `WRITE` access to the directory object.
3.  Write the Data Pump Import (`impdp`) command line that imports the data and objects from `sales.dmp` into the *new* `SALES_DEV` schema, using the appropriate remapping parameter.
4.  If the `SALES_APP` schema owner subsequently tries to drop the `DP_EXPORT_DIR` directory object, which privilege must they have been granted to succeed? (Cite your source)

Ready for your response!